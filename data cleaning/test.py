# process_news_html_test.py
# æ¸¬è©¦ç‰ˆï¼šåªè™•ç†å‰1000ç­†è³‡æ–™

import re
import json
import hashlib
import time
from pathlib import Path
from collections import Counter
from multiprocessing import Pool, cpu_count

import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm

# ==================== CONFIG ====================
NEWS_ROOT_FOLDER = r"C:\Users\a7385\OneDrive\Desktop\ç¨‹å¼æ¥æ¡ˆ\news"
OUT_CSV = "news_data_TEST_1000.csv"
OUT_QC_REPORT = "news_qc_report_TEST.json"
OUT_SAMPLE_HTML = "sample_records_TEST.json"  # å„²å­˜æ¨£æœ¬ç”¨æ–¼æª¢æŸ¥

TEST_LIMIT = 1000  # æ¸¬è©¦ç­†æ•¸é™åˆ¶
NUM_WORKERS = max(1, cpu_count() - 1)

# è‚¡ç¥¨é—œéµå­—
STOCK_KEYWORDS = [
    "è‚¡å¸‚", "è‚¡ç¥¨", "å°è‚¡", "ä¸Šå¸‚", "ä¸Šæ«ƒ", "æ«ƒè²·", "è­‰åˆ¸",
    "è²¡ç¶“", "è‚¡åƒ¹", "æŠ•è³‡", "æ³•äºº", "èè³‡", "èåˆ¸", "ç‡Ÿæ”¶",
    "ç²åˆ©", "è²¡å ±", "é™¤æ¬Š", "é™¤æ¯", "è‚¡æ±æœƒ", "ç›¤å‹¢", "æ¼²è·Œ",
    "å¤–è³‡", "æŠ•ä¿¡", "è‡ªç‡Ÿå•†", "å¤§ç›¤", "åŠ æ¬ŠæŒ‡æ•¸"
]
STOCK_PATTERN = re.compile('|'.join(map(re.escape, STOCK_KEYWORDS)))

MIN_CONTENT_LENGTH = 50

# ==================== FUNCTIONS ====================

def quick_stock_check(content_bytes):
    """å¿«é€Ÿæª¢æŸ¥æ˜¯å¦åŒ…å«è‚¡ç¥¨é—œéµå­—"""
    try:
        text_sample = content_bytes[:5000].decode('utf-8', errors='ignore')
        return bool(STOCK_PATTERN.search(text_sample))
    except:
        return False

def extract_date_fast(text):
    """å¿«é€Ÿæ—¥æœŸæå–"""
    if not text:
        return None
    
    patterns = [
        (r'(\d{4})-(\d{2})-(\d{2})', lambda m: m.group(0)),
        (r'(\d{4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥', 
         lambda m: f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"),
        (r'(\d{4})/(\d{2})/(\d{2})', lambda m: m.group(0).replace('/', '-')),
    ]
    
    for pattern, formatter in patterns:
        match = re.search(pattern, text[:500])
        if match:
            return formatter(match)
    
    return None

def clean_text_fast(text):
    """å¿«é€Ÿæ–‡å­—æ¸…ç†"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'(åˆ†äº«åˆ°|åˆ†äº«è‡³|è¿”å›åˆ—è¡¨|åˆ—å°|ä¸Šä¸€ç¯‡|ä¸‹ä¸€ç¯‡).*?(?=\s|$)', '', text)
    return text.strip()

def parse_html_fast(html_content, file_path):
    """å¿«é€Ÿ HTML è§£æ"""
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ç§»é™¤é›œè¨Šæ¨™ç±¤
        for tag in soup(['script', 'style', 'nav', 'footer', 'iframe']):
            tag.decompose()
        
        # æ¨™é¡Œ
        title = None
        if soup.find('h1', class_='title_'):
            title = soup.find('h1', class_='title_').get_text(strip=True)
        elif soup.find('meta', property='og:title'):
            title = soup.find('meta', property='og:title').get('content', '')
        elif soup.find('title'):
            title = soup.find('title').get_text(strip=True)
        
        # å…§å®¹
        content = None
        content_div = soup.find('div', class_='edit')
        if content_div:
            section = content_div.find('section')
            if section:
                content = section.get_text(separator=' ', strip=True)
        
        if not content:
            paragraphs = [p.get_text(separator=' ', strip=True) 
                         for p in soup.find_all(['p', 'article']) 
                         if len(p.get_text(strip=True)) > 100]
            if paragraphs:
                content = max(paragraphs, key=len)
        
        # æ—¥æœŸ
        date = None
        date_elem = soup.find('span', class_='date')
        if date_elem:
            date = extract_date_fast(date_elem.get_text())
        
        if not date:
            page_header = soup.get_text()[:1000]
            date = extract_date_fast(page_header)
        
        if not date:
            filename = file_path.name
            match = re.search(r'(\d{8})', filename)
            if match:
                d = match.group(1)
                date = f"{d[:4]}-{d[4:6]}-{d[6:8]}"
        
        # åˆ†é¡
        category = None
        breadcrumb = soup.find('ol', class_='bread_crumbs')
        if breadcrumb:
            cats = [li.get_text(strip=True) for li in breadcrumb.find_all('li')]
            if len(cats) > 1:
                category = cats[-2] if len(cats) > 2 else cats[-1]
        
        return {
            'title': clean_text_fast(title),
            'content': clean_text_fast(content),
            'date': date,
            'category': category
        }
    except Exception as e:
        return {'error': str(e)}

def process_single_file(file_path):
    """è™•ç†å–®å€‹æª”æ¡ˆ"""
    try:
        # è®€å–æª”æ¡ˆ
        with open(file_path, 'rb') as f:
            content_bytes = f.read()
        
        # æ—©æœŸéæ¿¾ï¼šå¿«é€Ÿæª¢æŸ¥æ˜¯å¦åŒ…å«è‚¡ç¥¨é—œéµå­—
        has_stock_keyword = quick_stock_check(content_bytes)
        
        # è§£ç¢¼
        html_content = content_bytes.decode('utf-8', errors='ignore')
        
        # è§£æ
        parsed = parse_html_fast(html_content, file_path)
        
        if 'error' in parsed:
            return {
                'status': 'parse_error',
                'file_path': str(file_path),
                'error': parsed['error']
            }
        
        if not parsed.get('title') or not parsed.get('content'):
            return {
                'status': 'missing_data',
                'file_path': str(file_path),
                'has_title': bool(parsed.get('title')),
                'has_content': bool(parsed.get('content'))
            }
        
        # é•·åº¦æª¢æŸ¥
        content_length = len(parsed['content'])
        if content_length < MIN_CONTENT_LENGTH:
            return {
                'status': 'too_short',
                'file_path': str(file_path),
                'content_length': content_length
            }
        
        # ç²¾ç¢ºè‚¡ç¥¨é—œéµå­—åŒ¹é…
        combined = f"{parsed['title']} {parsed['content']} {parsed['category'] or ''}"
        matches = STOCK_PATTERN.findall(combined)
        
        # è¨ˆç®—hash
        content_hash = hashlib.sha1(parsed['content'].encode('utf-8')).hexdigest()
        
        return {
            'status': 'success',
            'stock_related': len(matches) > 0,
            'quick_check': has_stock_keyword,
            'date': parsed['date'] or '',
            'title': parsed['title'],
            'content': parsed['content'],
            'content_length': content_length,
            'category': parsed['category'] or '',
            'keyword_matches': ','.join(set(matches)),
            'keyword_count': len(matches),
            'content_hash': content_hash,
            'file_path': str(file_path)
        }
    except Exception as e:
        return {
            'status': 'exception',
            'file_path': str(file_path),
            'error': str(e)
        }

# ==================== MAIN PROCESSING ====================

def process_news_files_test():
    """æ¸¬è©¦ç‰ˆä¸»è™•ç†å‡½æ•¸"""
    start_time = time.time()
    root = Path(NEWS_ROOT_FOLDER)
    
    if not root.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {root}")
        return
    
    # å¿«é€Ÿæƒææª”æ¡ˆ
    print("ğŸ” æƒææª”æ¡ˆä¸­...")
    all_files = [f for f in root.rglob("*") if f.is_file()]
    
    # é™åˆ¶æ¸¬è©¦æ•¸é‡
    test_files = all_files[:TEST_LIMIT]
    print(f"âœ“ ç¸½å…±æ‰¾åˆ° {len(all_files):,} å€‹æª”æ¡ˆ")
    print(f"ğŸ“‹ æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ {len(test_files)} å€‹æª”æ¡ˆ\n")
    
    if not test_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æª”æ¡ˆ")
        return
    
    # å¤šé€²ç¨‹è™•ç†
    print(f"ğŸš€ ä½¿ç”¨ {NUM_WORKERS} å€‹è™•ç†å™¨ä¸¦è¡Œè™•ç†...")
    all_results = []
    
    with Pool(NUM_WORKERS) as pool:
        results = pool.imap_unordered(process_single_file, test_files, chunksize=50)
        
        with tqdm(total=len(test_files), desc="è™•ç†é€²åº¦", unit="æª”æ¡ˆ") as pbar:
            for result in results:
                all_results.append(result)
                pbar.update(1)
    
    # çµ±è¨ˆåˆ†æ
    stats = {
        'total': len(all_results),
        'success': 0,
        'stock_related': 0,
        'parse_error': 0,
        'missing_data': 0,
        'too_short': 0,
        'exception': 0,
        'quick_check_match': 0,
        'keyword_counter': Counter(),
        'dates': []
    }
    
    valid_records = []
    sample_records = []  # ä¿å­˜å‰10ç­†ç”¨æ–¼æª¢æŸ¥
    
    for result in all_results:
        status = result.get('status')
        stats[status] = stats.get(status, 0) + 1
        
        if status == 'success':
            stats['success'] += 1
            
            if result['stock_related']:
                stats['stock_related'] += 1
                valid_records.append(result)
                
                # çµ±è¨ˆ
                if result['keyword_matches']:
                    keywords = result['keyword_matches'].split(',')
                    stats['keyword_counter'].update(keywords)
                if result['date']:
                    stats['dates'].append(result['date'])
                
                # ä¿å­˜å‰10ç­†å®Œæ•´æ¨£æœ¬
                if len(sample_records) < 10:
                    sample_records.append({
                        'file_path': result['file_path'],
                        'title': result['title'],
                        'content': result['content'][:500] + '...',  # åªä¿å­˜å‰500å­—
                        'date': result['date'],
                        'category': result['category'],
                        'keywords': result['keyword_matches'],
                        'content_length': result['content_length']
                    })
            
            if result['quick_check']:
                stats['quick_check_match'] += 1
    
    # å„²å­˜æ¨£æœ¬æª”æ¡ˆä¾›æª¢æŸ¥
    sample_path = Path.cwd() / OUT_SAMPLE_HTML
    with open(sample_path, 'w', encoding='utf-8') as f:
        json.dump(sample_records, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ æ¨£æœ¬è¨˜éŒ„å·²å„²å­˜: {sample_path}")
    
    # å»ºç«‹ DataFrame
    if valid_records:
        df = pd.DataFrame(valid_records)
        
        # é¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½
        output_columns = [
            'date', 'title', 'content', 'category', 
            'keyword_matches', 'keyword_count', 'content_length',
            'content_hash', 'file_path'
        ]
        df = df[output_columns]
        
        # å»é‡
        before = len(df)
        df = df.drop_duplicates(subset=['content_hash'], keep='first')
        after = len(df)
        
        # æ’åº
        df = df.sort_values(['date', 'title']).reset_index(drop=True)
        
        # å„²å­˜ CSV
        out_csv = Path.cwd() / OUT_CSV
        df.to_csv(out_csv, index=False, encoding='utf-8-sig')
        print(f"âœ“ CSV å·²å„²å­˜: {out_csv}")
        print(f"  - å»é‡å‰: {before} ç­†")
        print(f"  - å»é‡å¾Œ: {after} ç­†")
    else:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨ç›¸é—œæ–°è")
    
    # ç”Ÿæˆå ±å‘Š
    elapsed = time.time() - start_time
    qc_report = {
        'test_mode': True,
        'total_files_scanned': len(all_files),
        'test_sample_size': len(test_files),
        'processing_stats': {
            'success': stats['success'],
            'stock_related': stats['stock_related'],
            'parse_error': stats.get('parse_error', 0),
            'missing_data': stats.get('missing_data', 0),
            'too_short': stats.get('too_short', 0),
            'exception': stats.get('exception', 0)
        },
        'pass_rate': f"{stats['stock_related']/stats['total']*100:.2f}%" if stats['total'] > 0 else "0%",
        'after_dedup': len(df) if valid_records else 0,
        'quick_check_accuracy': f"{stats['quick_check_match']/stats['total']*100:.1f}%" if stats['total'] > 0 else "0%",
        'top_keywords': dict(stats['keyword_counter'].most_common(20)),
        'date_range': {
            'min': min(stats['dates']) if stats['dates'] else None,
            'max': max(stats['dates']) if stats['dates'] else None
        },
        'date_coverage': f"{len(stats['dates'])}/{stats['stock_related']} ({len(stats['dates'])/stats['stock_related']*100:.1f}%)" if stats['stock_related'] > 0 else "0/0",
        'elapsed_time': f"{elapsed:.1f} ç§’",
        'processing_speed': f"{stats['total']/elapsed:.1f} æª”æ¡ˆ/ç§’"
    }
    
    qc_path = Path.cwd() / OUT_QC_REPORT
    with open(qc_path, 'w', encoding='utf-8') as f:
        json.dump(qc_report, f, ensure_ascii=False, indent=2)
    
    # é¡¯ç¤ºæ‘˜è¦
    print("\n" + "="*70)
    print("ğŸ“ˆ æ¸¬è©¦çµæœæ‘˜è¦")
    print("="*70)
    print(f"æ¸¬è©¦æª”æ¡ˆæ•¸: {len(test_files):,} / {len(all_files):,}")
    print(f"\nè™•ç†ç‹€æ…‹:")
    print(f"  âœ“ æˆåŠŸè§£æ: {stats['success']}")
    print(f"  âœ“ è‚¡ç¥¨ç›¸é—œ: {stats['stock_related']} ({stats['stock_related']/stats['total']*100:.1f}%)")
    print(f"  âœ— è§£æéŒ¯èª¤: {stats.get('parse_error', 0)}")
    print(f"  âœ— è³‡æ–™ä¸å…¨: {stats.get('missing_data', 0)}")
    print(f"  âœ— å…§å®¹å¤ªçŸ­: {stats.get('too_short', 0)}")
    print(f"  âœ— å…¶ä»–éŒ¯èª¤: {stats.get('exception', 0)}")
    
    print(f"\nå¿«é€Ÿç¯©é¸æº–ç¢ºåº¦: {stats['quick_check_match']}/{stats['total']} ({stats['quick_check_match']/stats['total']*100:.1f}%)")
    
    if valid_records:
        print(f"\nå»é‡å¾Œè³‡æ–™: {len(df)} ç­†")
        
        if stats['dates']:
            date_coverage = len(stats['dates']) / stats['stock_related'] * 100
            print(f"æ—¥æœŸè¦†è“‹ç‡: {len(stats['dates'])}/{stats['stock_related']} ({date_coverage:.1f}%)")
            print(f"æ—¥æœŸç¯„åœ: {min(stats['dates'])} è‡³ {max(stats['dates'])}")
        
        print("\nğŸ”‘ å‰10å¤§é—œéµå­—:")
        for kw, count in stats['keyword_counter'].most_common(10):
            print(f"  {kw}: {count}")
        
        print("\nğŸ“„ æ¨£æœ¬è¨˜éŒ„ (å‰3ç­†):")
        for i, row in df.head(3).iterrows():
            print(f"\n  [{i+1}] {row['date']} - {row['title'][:50]}...")
            print(f"      å…§å®¹é•·åº¦: {row['content_length']} å­—")
            print(f"      é—œéµå­—: {row['keyword_matches']}")
    
    print(f"\nâ±ï¸  è™•ç†æ™‚é–“: {elapsed:.1f} ç§’ ({stats['total']/elapsed:.1f} æª”æ¡ˆ/ç§’)")
    print(f"âœ“ å ±å‘Šå·²å„²å­˜: {qc_path}")
    
    # ä¼°ç®—å®Œæ•´è™•ç†æ™‚é–“
    if len(all_files) > len(test_files):
        estimated_time = (elapsed / len(test_files)) * len(all_files)
        print(f"\nğŸ’¡ é ä¼°å®Œæ•´è™•ç†æ™‚é–“: {estimated_time/3600:.1f} å°æ™‚")

if __name__ == "__main__":
    print(f"ğŸ–¥ï¸  CPU æ ¸å¿ƒæ•¸: {cpu_count()} (ä½¿ç”¨ {NUM_WORKERS} å€‹è™•ç†å™¨)")
    print(f"ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ {TEST_LIMIT} å€‹æª”æ¡ˆ\n")
    process_news_files_test()